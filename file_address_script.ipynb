{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ddd0ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                              image                title  \\\n",
      "0          /images/team/Andrew.jpg  Andrew Teschendorff   \n",
      "1           /images/team/luoqi.jpg               Qi Luo   \n",
      "2       /images/team/tonghuige.jpg           Huige Tong   \n",
      "3     /images/team/guoxiaolong.jpg         Xiaolong Guo   \n",
      "4      /images/team/duzhaozhen.jpg          Zhaozhen Du   \n",
      "5           /images/team/jason.jpg  Jason tham han kiat   \n",
      "6          /images/team/naveed.jpg          Naveed Alam   \n",
      "7       /images/team/liangyuhu.jpg           Yuhu Liang   \n",
      "8    /images/team/wangkangying.jpg        Kangying Wang   \n",
      "9         /images/team/jinghan.jpg             Han Jing   \n",
      "10      /images/team/zhutianyu.jpg           Tianyu Zhu   \n",
      "11          /images/team/huxue.jpg               Xue Hu   \n",
      "12   /images/team/youchenglong.jpg        Chenglong You   \n",
      "13           /images/team/alok.jpg     Alok Kumar Maity   \n",
      "14    /images/team/zhengshijie.JPG         Shijie Zheng   \n",
      "15     /images/team/dongdanyue.jpg          Danyue Dong   \n",
      "16        /images/team/gaoyang.jpg             Yang Gao   \n",
      "17    /images/team/chenyuting3.jpg          Yuting Chen   \n",
      "18           /images/team/tian.jpg            Yuan Tian   \n",
      "19       /images/team/shenyiru.jpg            Yiru Shen   \n",
      "20         /images/team/linlin.jpg            Linlin Li   \n",
      "21         /images/team/limeng.jpg              Meng Li   \n",
      "22  /images/team/huangshoufeng.jpg       Shoufeng Huang   \n",
      "23        /images/team/yinming.jpg         Yinming Jiao   \n",
      "24        /images/team/gaorong.jpg             Rong Gao   \n",
      "25         /images/team/cuirui.png              Rui Cui   \n",
      "26    /images/team/chenweiyan.jpeg          Weiyan Chen   \n",
      "27      /images/team/wangning.jpeg            Ning Wang   \n",
      "28          /images/team/wahid.jpg      Md Wahiduzzaman   \n",
      "\n",
      "                  position  \n",
      "0   Principal Investigator  \n",
      "1            Ph.D. Student  \n",
      "2            Ph.D. Student  \n",
      "3            Ph.D. Student  \n",
      "4            Ph.D. Student  \n",
      "5            Ph.D. Student  \n",
      "6           Master Student  \n",
      "7                  Postdoc  \n",
      "8       Research Assistant  \n",
      "9            Ph.D. Student  \n",
      "10           Ph.D. Student  \n",
      "11          Master Student  \n",
      "12           Visit Student  \n",
      "13                 Postdoc  \n",
      "14           Ph.D. Student  \n",
      "15           Ph.D. Student  \n",
      "16           Ph.D. Student  \n",
      "17          Ph. D. Student  \n",
      "18           Ph.D. Student  \n",
      "19      Research Assistant  \n",
      "20      Research Assistant  \n",
      "21      Research Assistant  \n",
      "22      Research Assistant  \n",
      "23      Research Assistant  \n",
      "24         Staff Scientist  \n",
      "25      Research Assistant  \n",
      "26           Ph.D. Student  \n",
      "27           Ph.D. Student  \n",
      "28           Ph.D. Student  >\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def read_text(path: str) -> str:\n",
    "    return Path(path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "def extract_with_bs4(html: str):\n",
    "    try:\n",
    "        from bs4 import BeautifulSoup\n",
    "    except ImportError:\n",
    "        return None  # signal fallback\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    boxes = soup.select(\"div.memberbox\")  # matches <div class=\"... memberbox ...\">\n",
    "\n",
    "    rows = []\n",
    "    for box in boxes:\n",
    "        box_html = str(box)\n",
    "\n",
    "        # --- image src ---\n",
    "        img_src = None\n",
    "        # Try to find any tag with a src attribute (img OR malformed tags that still keep attrs)\n",
    "        tag_with_src = box.find(attrs={\"src\": True})\n",
    "        if tag_with_src and tag_with_src.get(\"src\"):\n",
    "            img_src = tag_with_src.get(\"src\")\n",
    "\n",
    "        # Regex fallback (more reliable for malformed HTML)\n",
    "        if not img_src:\n",
    "            m = re.search(r'src\\s*=\\s*\"([^\"]+)\"', box_html)\n",
    "            if m:\n",
    "                img_src = m.group(1)\n",
    "\n",
    "        # --- title ---\n",
    "        title = None\n",
    "        off_tag = box.select_one(\".off\")\n",
    "        if off_tag:\n",
    "            title = off_tag.get_text(strip=True)\n",
    "\n",
    "        # Fallback: sometimes name is in .head with no <a class=\"off\">\n",
    "        if not title:\n",
    "            head = box.select_one(\".head\")\n",
    "            if head:\n",
    "                title = head.get_text(\" \", strip=True)\n",
    "\n",
    "        # --- position ---\n",
    "        position = None\n",
    "        note = box.select_one(\"p.note\")\n",
    "        if note:\n",
    "            position = note.get_text(\" \", strip=True)\n",
    "\n",
    "        # Skip completely empty rows\n",
    "        if any([img_src, title, position]):\n",
    "            rows.append(\n",
    "                {\"image\": img_src, \"title\": title, \"position\": position}\n",
    "            )\n",
    "\n",
    "    return rows\n",
    "\n",
    "def extract_with_regex(html: str):\n",
    "    # Split by memberbox blocks (works even if HTML is messy)\n",
    "    parts = re.split(r'<div\\s+class=\"[^\"]*\\bmemberbox\\b[^\"]*\"\\s*>', html, flags=re.I)\n",
    "    rows = []\n",
    "    for part in parts[1:]:\n",
    "        # Try to limit to roughly this box: cut at next memberbox start if present\n",
    "        part = re.split(r'<div\\s+class=\"[^\"]*\\bmemberbox\\b[^\"]*\"\\s*>', part, maxsplit=1, flags=re.I)[0]\n",
    "\n",
    "        img = None\n",
    "        m = re.search(r'src\\s*=\\s*\"([^\"]+/images/team/[^\"]+)\"', part, flags=re.I)\n",
    "        if m:\n",
    "            img = m.group(1)\n",
    "        else:\n",
    "            m = re.search(r'src\\s*=\\s*\"([^\"]+)\"', part, flags=re.I)\n",
    "            if m:\n",
    "                img = m.group(1)\n",
    "\n",
    "        title = None\n",
    "        m = re.search(r'class\\s*=\\s*\"off\"\\s*>\\s*([^<]+)', part, flags=re.I)\n",
    "        if m:\n",
    "            title = m.group(1).strip()\n",
    "\n",
    "        position = None\n",
    "        m = re.search(r'<p\\s+class\\s*=\\s*\"note\"\\s*>\\s*([^<]+)\\s*</p>', part, flags=re.I)\n",
    "        if m:\n",
    "            position = m.group(1).strip()\n",
    "\n",
    "        if any([img, title, position]):\n",
    "            rows.append({\"image\": img, \"title\": title, \"position\": position})\n",
    "\n",
    "    return rows\n",
    "\n",
    "def main():\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python extract_team.py <input.html> [output.csv]\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    in_path = '/Users/fantastic-lin/Documents/Andrew/Lab website/aet21.github.io-master/team/index.html'\n",
    "    out_path = '/Users/fantastic-lin/Documents/Andrew/Lab website/aet21.github.io-master/Lin/team_members.csv'\n",
    "\n",
    "    html = read_text(in_path)\n",
    "\n",
    "    rows = extract_with_bs4(html)\n",
    "    if rows is None:  # bs4 not installed\n",
    "        rows = extract_with_regex(html)\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"image\", \"title\", \"position\"])\n",
    "\n",
    "    # Optional: drop duplicates / empty titles\n",
    "    df = df.dropna(how=\"all\")\n",
    "    df = df.drop_duplicates()\n",
    "    print(df.head)\n",
    "    #df.to_csv(out_path, index=False)\n",
    "    #print(f\"Extracted {len(df)} rows -> {out_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a2c504",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import math\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "SUPPORTED = {\".jpg\", \".jpeg\", \".png\", \".webp\", \".tif\", \".tiff\", \".bmp\"}\n",
    "\n",
    "def infer_ratio(image_paths):\n",
    "    \"\"\"Infer a common aspect ratio r = W/H that minimizes total crop loss (median in log-space).\"\"\"\n",
    "    logs = []\n",
    "    for p in image_paths:\n",
    "        with Image.open(p) as im:\n",
    "            w, h = im.size\n",
    "        if w > 0 and h > 0:\n",
    "            logs.append(math.log(w / h))\n",
    "    if not logs:\n",
    "        raise ValueError(\"No valid images found to infer ratio.\")\n",
    "    logs.sort()\n",
    "    mid = len(logs) // 2\n",
    "    median_log = logs[mid] if len(logs) % 2 == 1 else (logs[mid - 1] + logs[mid]) / 2\n",
    "    return math.exp(median_log)\n",
    "\n",
    "def center_crop_to_ratio(im, ratio):\n",
    "    \"\"\"\n",
    "    Center-crop image to target ratio (W/H) without resizing.\n",
    "    Keeps the largest possible crop that matches the ratio.\n",
    "    \"\"\"\n",
    "    w, h = im.size\n",
    "    current = w / h\n",
    "\n",
    "    if abs(current - ratio) < 1e-9:\n",
    "        return im  # already matches\n",
    "\n",
    "    if current > ratio:\n",
    "        # Image is too wide -> crop width\n",
    "        new_w = int(round(ratio * h))\n",
    "        new_h = h\n",
    "    else:\n",
    "        # Image is too tall -> crop height\n",
    "        new_w = w\n",
    "        new_h = int(round(w / ratio))\n",
    "\n",
    "    left = (w - new_w) // 2\n",
    "    top = (h - new_h) // 2\n",
    "    right = left + new_w\n",
    "    bottom = top + new_h\n",
    "\n",
    "    return im.crop((left, top, right, bottom))\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser(description=\"Infer a common aspect ratio and center-crop all images in a directory.\")\n",
    "    ap.add_argument(\"input_dir\", help=\"Directory containing images\")\n",
    "    ap.add_argument(\"-o\", \"--output_dir\", default=\"cropped\", help=\"Output directory (default: cropped)\")\n",
    "    ap.add_argume_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bb3a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32 images.\n",
      "Inferred target ratio (W/H): 0.770489\n",
      "Approx ratio ≈ 37:48 (err=0.00034403)\n",
      "Done. Cropped 32 images -> /Users/fantastic-lin/Documents/Andrew/Lab website/aet21.github.io-master/Lin/cropped_imagegs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(        filename                                         input_path  \\\n",
       " 0     linlin.jpg  /Users/fantastic-lin/Documents/Andrew/Lab webs...   \n",
       " 1  zhutianyu.jpg  /Users/fantastic-lin/Documents/Andrew/Lab webs...   \n",
       " 2     Andrew.jpg  /Users/fantastic-lin/Documents/Andrew/Lab webs...   \n",
       " 3  liangyuhu.jpg  /Users/fantastic-lin/Documents/Andrew/Lab webs...   \n",
       " 4  tonghuige.jpg  /Users/fantastic-lin/Documents/Andrew/Lab webs...   \n",
       " \n",
       "                                          output_path  orig_w  orig_h  \\\n",
       " 0  /Users/fantastic-lin/Documents/Andrew/Lab webs...    3072    3950   \n",
       " 1  /Users/fantastic-lin/Documents/Andrew/Lab webs...     960    1280   \n",
       " 2  /Users/fantastic-lin/Documents/Andrew/Lab webs...     720    1080   \n",
       " 3  /Users/fantastic-lin/Documents/Andrew/Lab webs...     668     950   \n",
       " 4  /Users/fantastic-lin/Documents/Andrew/Lab webs...     831    1080   \n",
       " \n",
       "    orig_ratio  crop_w  crop_h  target_ratio  \n",
       " 0    0.777722    3043    3950      0.770489  \n",
       " 1    0.750000     960    1246      0.770489  \n",
       " 2    0.666667     720     934      0.770489  \n",
       " 3    0.703158     668     867      0.770489  \n",
       " 4    0.769444     831    1079      0.770489  ,\n",
       " 'Saved CSV: /Users/fantastic-lin/Documents/Andrew/Lab website/aet21.github.io-master/Lin/cropped_imagegs/crop_summary.csv',\n",
       " Empty DataFrame\n",
       " Columns: []\n",
       " Index: [])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================\n",
    "# Batch center-crop images in Jupyter\n",
    "# - infers a common aspect ratio (W/H) that minimizes overall cropping\n",
    "# - center-crops each image to that ratio (no resizing)\n",
    "# - saves outputs + writes a CSV summary\n",
    "# - FIXED: safe saving for JPEG (no RGBA)\n",
    "# ============================\n",
    "\n",
    "#Inferred target ratio (W/H): 0.770489\n",
    "#Approx ratio ≈ 37:48 (err=0.00034403)\n",
    "\n",
    "#!pip -q install pillow pandas\n",
    "\n",
    "from pathlib import Path\n",
    "import math\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "# ---- 1) CONFIG: set your paths here ----\n",
    "INPUT_DIR = Path(\"/Users/fantastic-lin/Documents/Andrew/Lab website/aet21.github.io-master/images/team\")   # <-- CHANGE THIS\n",
    "OUTPUT_DIR = Path(\"/Users/fantastic-lin/Documents/Andrew/Lab website/aet21.github.io-master/Lin/cropped_imagegs\")\n",
    "RECURSIVE = False\n",
    "SUPPORTED = {\".jpg\", \".jpeg\", \".png\", \".webp\", \".tif\", \".tiff\", \".bmp\"}\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- 2) Collect image files ----\n",
    "if RECURSIVE:\n",
    "    paths = [p for p in INPUT_DIR.rglob(\"*\") if p.is_file() and p.suffix.lower() in SUPPORTED]\n",
    "else:\n",
    "    paths = [p for p in INPUT_DIR.iterdir() if p.is_file() and p.suffix.lower() in SUPPORTED]\n",
    "\n",
    "if not paths:\n",
    "    raise ValueError(f\"No supported images found in: {INPUT_DIR}\")\n",
    "\n",
    "print(f\"Found {len(paths)} images.\")\n",
    "\n",
    "# ---- 3) Infer a common ratio (W/H) ----\n",
    "def infer_ratio(image_paths):\n",
    "    logs = []\n",
    "    for p in image_paths:\n",
    "        with Image.open(p) as im:\n",
    "            w, h = im.size\n",
    "        if w > 0 and h > 0:\n",
    "            logs.append(math.log(w / h))\n",
    "    if not logs:\n",
    "        raise ValueError(\"No valid images found to infer ratio.\")\n",
    "    logs.sort()\n",
    "    mid = len(logs) // 2\n",
    "    median_log = logs[mid] if len(logs) % 2 == 1 else (logs[mid - 1] + logs[mid]) / 2\n",
    "    return math.exp(median_log)\n",
    "\n",
    "ratio = infer_ratio(paths)\n",
    "print(f\"Inferred target ratio (W/H): {ratio:.6f}\")\n",
    "\n",
    "# Optional: pretty W:H approximation\n",
    "def ratio_to_wh(r, max_den=60):\n",
    "    best_w, best_h, best_err = 1, 1, float(\"inf\")\n",
    "    for h in range(1, max_den + 1):\n",
    "        w = round(r * h)\n",
    "        err = abs(r - (w / h))\n",
    "        if err < best_err:\n",
    "            best_w, best_h, best_err = w, h, err\n",
    "    return best_w, best_h, best_err\n",
    "\n",
    "w_approx, h_approx, err = ratio_to_wh(ratio, max_den=60)\n",
    "print(f\"Approx ratio ≈ {w_approx}:{h_approx} (err={err:.6g})\")\n",
    "\n",
    "# ---- 4) Center-crop function ----\n",
    "def center_crop_to_ratio(im, ratio):\n",
    "    w, h = im.size\n",
    "    cur = w / h\n",
    "\n",
    "    if abs(cur - ratio) < 1e-9:\n",
    "        return im.copy(), (w, h)\n",
    "\n",
    "    if cur > ratio:\n",
    "        # too wide -> crop width\n",
    "        new_w = int(round(ratio * h))\n",
    "        new_h = h\n",
    "    else:\n",
    "        # too tall -> crop height\n",
    "        new_w = w\n",
    "        new_h = int(round(w / ratio))\n",
    "\n",
    "    left = (w - new_w) // 2\n",
    "    top = (h - new_h) // 2\n",
    "    right = left + new_w\n",
    "    bottom = top + new_h\n",
    "\n",
    "    return im.crop((left, top, right, bottom)), (new_w, new_h)\n",
    "\n",
    "# ---- 5) Safe save helper (fixes RGBA->JPEG issue) ----\n",
    "def prepare_for_saving(im, out_path: Path, background=(255, 255, 255)):\n",
    "    \"\"\"\n",
    "    Ensure image mode is compatible with the target format based on extension.\n",
    "    - JPEG: must be RGB (no alpha). If has alpha, composite on background.\n",
    "    - PNG/WebP: can keep RGBA.\n",
    "    \"\"\"\n",
    "    ext = out_path.suffix.lower()\n",
    "\n",
    "    # If target is JPEG, force RGB\n",
    "    if ext in {\".jpg\", \".jpeg\"}:\n",
    "        if im.mode in (\"RGBA\", \"LA\") or (\"transparency\" in im.info):\n",
    "            # Composite onto a solid background\n",
    "            rgba = im.convert(\"RGBA\")\n",
    "            bg = Image.new(\"RGBA\", rgba.size, background + (255,))\n",
    "            im = Image.alpha_composite(bg, rgba).convert(\"RGB\")\n",
    "        else:\n",
    "            im = im.convert(\"RGB\")\n",
    "        return im\n",
    "\n",
    "    # Non-JPEG targets: generally safe, but normalize some modes\n",
    "    if im.mode == \"P\":\n",
    "        # palette -> RGBA to preserve transparency if any\n",
    "        im = im.convert(\"RGBA\")\n",
    "    elif im.mode == \"CMYK\":\n",
    "        im = im.convert(\"RGB\")\n",
    "\n",
    "    return im\n",
    "\n",
    "# ---- 6) Crop all images + save + summary ----\n",
    "summary = []\n",
    "failures = []\n",
    "\n",
    "for p in paths:\n",
    "    try:\n",
    "        with Image.open(p) as im:\n",
    "            orig_w, orig_h = im.size\n",
    "            cropped, (new_w, new_h) = center_crop_to_ratio(im, ratio)\n",
    "\n",
    "            out_path = OUTPUT_DIR / p.name\n",
    "            cropped_to_save = prepare_for_saving(cropped, out_path)\n",
    "\n",
    "            save_kwargs = {}\n",
    "            if out_path.suffix.lower() in {\".jpg\", \".jpeg\"}:\n",
    "                save_kwargs = {\"quality\": 95, \"optimize\": True}\n",
    "\n",
    "            cropped_to_save.save(out_path, **save_kwargs)\n",
    "\n",
    "            summary.append({\n",
    "                \"filename\": p.name,\n",
    "                \"input_path\": str(p),\n",
    "                \"output_path\": str(out_path),\n",
    "                \"orig_w\": orig_w,\n",
    "                \"orig_h\": orig_h,\n",
    "                \"orig_ratio\": (orig_w / orig_h) if orig_h else None,\n",
    "                \"crop_w\": new_w,\n",
    "                \"crop_h\": new_h,\n",
    "                \"target_ratio\": ratio,\n",
    "            })\n",
    "    except Exception as e:\n",
    "        failures.append({\"filename\": p.name, \"input_path\": str(p), \"error\": repr(e)})\n",
    "\n",
    "print(f\"Done. Cropped {len(summary)} images -> {OUTPUT_DIR.resolve()}\")\n",
    "if failures:\n",
    "    print(f\"⚠️ Failed on {len(failures)} images. See failures dataframe below.\")\n",
    "\n",
    "df = pd.DataFrame(summary)\n",
    "csv_path = OUTPUT_DIR / \"crop_summary.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "fail_df = pd.DataFrame(failures)\n",
    "df.head(), f\"Saved CSV: {csv_path}\", fail_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6f26b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected headers: ['image', 'title', 'position', 'alumni', 'role_key']\n",
      "Done. Wrote 31 files to: /Users/fantastic-lin/Documents/Andrew/Lab_website/aet21.github.io-master/_people\n"
     ]
    }
   ],
   "source": [
    "import csv, re, textwrap\n",
    "from pathlib import Path\n",
    "\n",
    "INPUT_CSV = \"/Users/fantastic-lin/Documents/Andrew/Lab_website/aet21.github.io-master/Lin/team_members.csv\"          # <-- your CSV filename/path\n",
    "OUTPUT_DIR = \"/Users/fantastic-lin/Documents/Andrew/Lab_website/aet21.github.io-master/_people\"        # <-- folder to create\n",
    "\n",
    "FIXED_CATEGORY = \"people\"\n",
    "\n",
    "def slugify(s: str) -> str:\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r\"[’'`]\", \"\", s)\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \"-\", s)\n",
    "    s = re.sub(r\"-{2,}\", \"-\", s).strip(\"-\")\n",
    "    return s or \"person\"\n",
    "\n",
    "out_dir = Path(OUTPUT_DIR)\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "used = {}  # handle duplicate names like \"Lin Lin\" / \"Linlin Li\"\n",
    "created = []\n",
    "\n",
    "with open(INPUT_CSV, \"r\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "    reader = csv.DictReader(f, delimiter=\",\")\n",
    "    print(\"Detected headers:\", reader.fieldnames)\n",
    "\n",
    "    for row in reader:\n",
    "        # normalize keys (avoid issues like \" title \" vs \"title\")\n",
    "        row = {(k or \"\").strip().lower(): (v or \"\").strip() for k, v in row.items()}\n",
    "\n",
    "        title = row[\"title\"]\n",
    "        position = row[\"position\"]\n",
    "        image = \"/people\"+row[\"image\"]\n",
    "        alumni = row[\"alumni\"]\n",
    "        role_key = row[\"role_key\"]\n",
    "\n",
    "        base = slugify(title)\n",
    "        used[base] = used.get(base, 0) + 1\n",
    "        slug = base if used[base] == 1 else f\"{base}-{used[base]}\"\n",
    "\n",
    "        filename = f\"{slug}.md\"  # or f\"2026-01-09-{slug}.md\" if you want date prefix\n",
    "        content = textwrap.dedent(f\"\"\"\\\n",
    "        ---\n",
    "        title: {title}\n",
    "        categories:\n",
    "          - {FIXED_CATEGORY}\n",
    "        position: {position}\n",
    "        image: {image}\n",
    "        alumni: {alumni}\n",
    "        role_key: {role_key}\n",
    "        ---\n",
    "        \"\"\")\n",
    "\n",
    "        (out_dir / filename).write_text(content, encoding=\"utf-8\")\n",
    "        created.append(filename)\n",
    "\n",
    "print(f\"Done. Wrote {len(created)} files to: {out_dir.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb692d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AutosurveyGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
